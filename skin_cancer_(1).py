# -*- coding: utf-8 -*-
"""Skin_Cancer (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mBRPD4m8uZZ9haumf6FJ5Bn3pqDem470
"""

from google.colab import files
files.upload()   # Upload your kaggle.json file

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000

!kaggle datasets download -d tschandl/ham10000-lesion-segmentations

!mkdir -p HAM10000/masks
!unzip ham10000-lesion-segmentations.zip -d HAM10000/masks

!unzip skin-cancer-mnist-ham10000.zip -d HAM10000

import os, shutil
from tqdm import tqdm

os.makedirs("HAM10000/all_images", exist_ok=True)

for folder in ["HAM10000/HAM10000_images_part_1", "HAM10000/HAM10000_images_part_2"]:
    for file in tqdm(os.listdir(folder)):
        if file.endswith(".jpg"):
            shutil.copy(f"{folder}/{file}", "HAM10000/all_images/")

import pandas as pd

df = pd.read_csv("HAM10000/HAM10000_metadata.csv")
df.head()

print(df.columns)

import os

masks = os.listdir("HAM10000/masks")
print("Total masks:", len(masks))
print("Sample masks:", masks[:10])

!rm -rf /content/HAM10000/masks
!mkdir -p /content/HAM10000/masks

!unzip ham10000-lesion-segmentations.zip -d /content/HAM10000/masks

print("HAM10000/masks/HAM10000_segmentations_lesion_tschandl/")

import os, shutil

root = "/content/HAM10000/masks"
sub = os.path.join(root, "HAM10000_segmentations_lesion_tschandl")

# Move all PNG files to parent directory
for file in os.listdir(sub):
    if file.endswith(".png"):
        shutil.move(os.path.join(sub, file), root)

# Remove the empty folder
shutil.rmtree(sub)

print("Masks moved successfully!")

import os

mask_dir = "/content/HAM10000/masks"
print("Total masks:", len(os.listdir(mask_dir)))
print("Sample masks:", os.listdir(mask_dir)[:10])

import os
import cv2
import numpy as np
from tqdm import tqdm
import shutil
import random

IMAGE_DIR = "/content/HAM10000/all_images/"
MASK_DIR = "/content/HAM10000/masks/"
LABEL_DIR = "/content/HAM10000/labels/"

os.makedirs(LABEL_DIR, exist_ok=True)

def mask_to_bbox(mask):
    coords = cv2.findNonZero(mask)
    if coords is None:
        return None
    x, y, w, h = cv2.boundingRect(coords)
    return x, y, w, h

image_files = [f for f in os.listdir(IMAGE_DIR) if f.endswith(".jpg")]
print("Total images found:", len(image_files))

label_count = 0

for img_name in tqdm(image_files):

    img_path = os.path.join(IMAGE_DIR, img_name)
    mask_path = os.path.join(MASK_DIR, img_name.replace(".jpg", "_segmentation.png"))

    # Mask must exist
    if not os.path.exists(mask_path):
        continue

    # Load image
    image = cv2.imread(img_path)
    H, W, _ = image.shape

    # Load mask (grayscale)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
    if mask is None:
        continue

    # Get bounding box
    bbox = mask_to_bbox(mask)
    if bbox is None:
        continue

    x, y, w, h = bbox

    # Normalize to YOLO format
    x_center = (x + w / 2) / W
    y_center = (y + h / 2) / H
    width = w / W
    height = h / H

    # Save YOLO label
    label_path = os.path.join(LABEL_DIR, img_name.replace(".jpg", ".txt"))
    with open(label_path, "w") as f:
        f.write(f"0 {x_center} {y_center} {width} {height}")

    label_count += 1

print("Total labels created:", label_count)

YOLO_DIR = "/content/HAM10000/yolo_dataset"
os.makedirs(f"{YOLO_DIR}/images/train", exist_ok=True)
os.makedirs(f"{YOLO_DIR}/images/valid", exist_ok=True)
os.makedirs(f"{YOLO_DIR}/labels/train", exist_ok=True)
os.makedirs(f"{YOLO_DIR}/labels/valid", exist_ok=True)

all_images = [f for f in os.listdir(IMAGE_DIR) if f.endswith(".jpg")]
random.shuffle(all_images)

train_split = int(0.8 * len(all_images))
train_imgs = all_images[:train_split]
valid_imgs = all_images[train_split:]

def copy_data(files, split):
    for img_name in tqdm(files):
        # Copy image
        shutil.copy(
            os.path.join(IMAGE_DIR, img_name),
            f"{YOLO_DIR}/images/{split}/{img_name}"
        )

        # Copy label if exists
        label_file = img_name.replace(".jpg", ".txt")
        label_src = os.path.join(LABEL_DIR, label_file)

        if os.path.exists(label_src):
            shutil.copy(
                label_src,
                f"{YOLO_DIR}/labels/{split}/{label_file}"
            )

copy_data(train_imgs, "train")
copy_data(valid_imgs, "valid")

print("YOLO dataset ready!")

yaml_content = """
train: /content/HAM10000/yolo_dataset/images/train
val: /content/HAM10000/yolo_dataset/images/valid

nc: 1
names: ["lesion"]
"""

with open("/content/HAM10000/yolo_dataset/data.yaml", "w") as f:
    f.write(yaml_content)

print("data.yaml created.")

print("Train images:", len(os.listdir("/content/HAM10000/yolo_dataset/images/train")))
print("Train labels:", len(os.listdir("/content/HAM10000/yolo_dataset/labels/train")))
print("Valid images:", len(os.listdir("/content/HAM10000/yolo_dataset/images/valid")))
print("Valid labels:", len(os.listdir("/content/HAM10000/yolo_dataset/labels/valid")))

!pip install ultralytics

from ultralytics import YOLO

model = YOLO("yolov8n.pt")

results = model.train(
    data="/content/HAM10000/yolo_dataset/data.yaml",
    epochs=10,
    imgsz=512,
    batch=8,
    workers=2,
    optimizer='Adam',
    project="FAST_YOLO",
    name="lesion_detector_fast",
    device=0,
)

from ultralytics import YOLO

model = YOLO('/content/FAST_YOLO/lesion_detector_fast/weights/best.pt')
model.predict('/content/HAM10000/all_images/ISIC_0033060.jpg', save=True)

from ultralytics import YOLO
import matplotlib.pyplot as plt
import cv2

# Load model
model = YOLO("/content/FAST_YOLO/lesion_detector_fast/weights/best.pt")

# Image path
img_path = "/content/HAM10000/all_images/ISIC_0024306.jpg"

# Predict and save image
results = model.predict(img_path, save=True)

# Get saved image path
saved_path = results[0].save_dir + "/" + results[0].path.split("/")[-1]

# Read and show image
img = cv2.imread(saved_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

plt.figure(figsize=(8, 8))
plt.imshow(img)
plt.axis("off")
plt.show()

from ultralytics import YOLO
import pandas as pd
import cv2
import os
from tqdm import tqdm

# Load trained YOLO model
yolo_model = YOLO("/content/FAST_YOLO/lesion_detector_fast/weights/best.pt")

# Load metadata
df = pd.read_csv("/content/HAM10000/HAM10000_metadata.csv")

# Create dictionary: image_id -> dx label
label_map = dict(zip(df['image_id'], df['dx']))

print("Metadata loaded. Total entries:", len(label_map))

BASE_OUT = "/content/EfficientNet_dataset"
classes = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']

for c in classes:
    os.makedirs(f"{BASE_OUT}/{c}", exist_ok=True)

IMAGE_DIR = "/content/HAM10000/all_images/"

for img_name in tqdm(os.listdir(IMAGE_DIR)):
    if not img_name.endswith(".jpg"):
        continue

    image_path = os.path.join(IMAGE_DIR, img_name)

    # Extract image_id (without extension)
    image_id = img_name.replace(".jpg", "")

    # Skip if metadata does not have label
    if image_id not in label_map:
        continue

    label = label_map[image_id]   # e.g., "mel", "nv"

    # Run YOLO detection
    results = yolo_model(image_path)[0]

    if len(results.boxes) == 0:
        # If YOLO fails to detect, skip
        continue

    # Get first bounding box
    box = results.boxes.xyxy[0].cpu().numpy().astype(int)
    x1, y1, x2, y2 = box

    # Read full image
    img = cv2.imread(image_path)

    # Crop lesion
    crop = img[y1:y2, x1:x2]

    # Save cropped region into class folder
    out_path = f"{BASE_OUT}/{label}/{img_name}"
    cv2.imwrite(out_path, crop)

!pip install torch torchvision timm

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import timm
from tqdm import tqdm
import os

DATA_DIR = "/content/EfficientNet_dataset"  # dataset created in Phase 4
BATCH_SIZE = 32
NUM_CLASSES = 7
EPOCHS = 10  # can increase to 25 for better accuracy
LR = 1e-4
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print("Using device:", DEVICE)

train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
])

valid_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

train_set = datasets.ImageFolder(DATA_DIR, transform=train_transform)
valid_set = datasets.ImageFolder(DATA_DIR, transform=valid_transform)

train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)

print("Classes:", train_set.classes)

model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=NUM_CLASSES)
model = model.to(DEVICE)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LR)

best_accuracy = 0

for epoch in range(EPOCHS):
    print(f"\nEpoch {epoch+1}/{EPOCHS}")

    # Training
    model.train()
    running_loss = 0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(DEVICE), labels.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    train_loss = running_loss / len(train_loader)

    # Validation
    model.eval()
    correct = 0
    total = 0
    val_loss = 0

    with torch.no_grad():
        for images, labels in valid_loader:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    val_accuracy = correct / total * 100
    val_loss = val_loss / len(valid_loader)

    print(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%")

    # Save best model
    if val_accuracy > best_accuracy:
        best_accuracy = val_accuracy
        torch.save(model.state_dict(), "efficientnet_best.pth")
        print("Model Saved!")

print("Training completed.")
print(f"Best validation accuracy: {best_accuracy:.2f}%")
print("Model saved as efficientnet_best.pth")

from ultralytics import YOLO
import torch
import torch.nn as nn
from torchvision import transforms
import timm
import cv2
import numpy as np
from PIL import Image

yolo_model = YOLO("/content/FAST_YOLO/lesion_detector_fast/weights/best.pt")

# Load EfficientNet model
num_classes = 7
eff_model = timm.create_model('efficientnet_b0', pretrained=False, num_classes=num_classes)
eff_model.load_state_dict(torch.load("/content/efficientnet_best.pth", map_location="cpu"))
eff_model.eval()

class_names = [
    "Actinic Keratoses (akiec)",
    "Basal Cell Carcinoma (bcc)",
    "Benign Keratosis-like Lesions (bkl)",
    "Dermatofibroma (df)",
    "Melanoma (mel)",
    "Melanocytic Nevi (nv)",
    "Vascular Lesions (vasc)"
]

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def predict_skin_lesion(image_path):
    # --- YOLO detection ---
    yolo_results = yolo_model(image_path)[0]

    if len(yolo_results.boxes) == 0:
        return "No lesion detected by YOLO."

    # Use first detected bounding box
    box = yolo_results.boxes.xyxy[0].cpu().numpy().astype(int)
    x1, y1, x2, y2 = box

    # Load image
    img = cv2.imread(image_path)
    crop = img[y1:y2, x1:x2]

    # Convert to PIL for EfficientNet
    pil_img = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))
    tensor_img = transform(pil_img).unsqueeze(0)

    # --- EfficientNet prediction ---
    with torch.no_grad():
        outputs = eff_model(tensor_img)
        probs = torch.nn.functional.softmax(outputs, dim=1)
        confidence, predicted = torch.max(probs, 1)

    class_name = class_names[predicted.item()]
    conf_percent = confidence.item() * 100

    # Draw result on original image
    result_img = img.copy()
    cv2.rectangle(result_img, (x1,y1), (x2,y2), (0,255,0), 2)
    cv2.putText(result_img, f"{class_name} ({conf_percent:.1f}%)",
                (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,
                0.8, (0,255,0), 2)

    # Save and display final output
    cv2.imwrite("final_prediction.jpg", result_img)

    return f"Prediction: {class_name} ({conf_percent:.2f}%)"

result = predict_skin_lesion("/content/HAM10000/all_images/ISIC_0024306.jpg")
print(result)

from google.colab.patches import cv2_imshow
output_img = cv2.imread("final_prediction.jpg")
cv2_imshow(output_img)

from google.colab import files
from google.colab.patches import cv2_imshow
import cv2

# Step 1: Upload Image
uploaded = files.upload()
image_path = list(uploaded.keys())[0]
print("Uploaded:", image_path)

# Step 2: Run Prediction
result = predict_skin_lesion(image_path)
print("Prediction:", result)

# Step 3: Display Original Image
print("\nOriginal Image:")
img = cv2.imread(image_path)
cv2_imshow(img)

# Step 4: Display Processed Output Image
print("\nYOLO + EfficientNet Output:")
output_img = cv2.imread("final_prediction.jpg")
cv2_imshow(output_img)

